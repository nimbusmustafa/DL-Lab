{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Implement L2 regularization on cat-dog classification neural network. Train the model on the dataset, and observe the impact of the regularization on the weight parameters. (Do not use data augmentation).\n",
    "a. L2 regularization using optimizer’s weight decay\n",
    "b. L2 regularization using loop to find L2 norm of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['cats', 'dogs'], Number: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations (No Data Augmentation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset (Change path accordingly)\n",
    "train_dataset = datasets.ImageFolder(root=\"/home/mustafa/dllab/w7prac/cats_and_dogs_filtered/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/home/mustafa/dllab/w7prac/cats_and_dogs_filtered/validation\", transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check number of classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Classes: {train_dataset.classes}, Number: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CatDogCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=32768, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CatDogCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CatDogCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = CatDogCNN(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6976\n",
      "Epoch [2/10], Loss: 0.6924\n",
      "Epoch [3/10], Loss: 0.6836\n",
      "Epoch [4/10], Loss: 0.6557\n",
      "Epoch [5/10], Loss: 0.6162\n",
      "Epoch [6/10], Loss: 0.5897\n",
      "Epoch [7/10], Loss: 0.5386\n",
      "Epoch [8/10], Loss: 0.5275\n",
      "Epoch [9/10], Loss: 0.4832\n",
      "Epoch [10/10], Loss: 0.4551\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer (L2 regularization using weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 Regularization\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4429\n",
      "Epoch [2/10], Loss: 0.4066\n",
      "Epoch [3/10], Loss: 0.3618\n",
      "Epoch [4/10], Loss: 0.3428\n",
      "Epoch [5/10], Loss: 0.3034\n",
      "Epoch [6/10], Loss: 0.2703\n",
      "Epoch [7/10], Loss: 0.2530\n",
      "Epoch [8/10], Loss: 0.2008\n",
      "Epoch [9/10], Loss: 0.1955\n",
      "Epoch [10/10], Loss: 0.1623\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer without weight_decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "lambda_l2 = 1e-4  # L2 Regularization factor\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute L2 norm manually\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss += lambda_l2 * l2_norm\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Norms with weight_decay: {'conv1.weight': 3.1693837642669678, 'conv1.bias': 0.5760012269020081, 'conv2.weight': 5.506408214569092, 'conv2.bias': 0.30440694093704224, 'conv3.weight': 4.9195122718811035, 'conv3.bias': 0.25522398948669434, 'fc1.weight': 21.993850708007812, 'fc1.bias': 0.22109541296958923, 'fc2.weight': 1.2369147539138794, 'fc2.bias': 0.03193413093686104}\n"
     ]
    }
   ],
   "source": [
    "def compute_weight_norms(model):\n",
    "    norms = {name: param.norm().item() for name, param in model.named_parameters() if param.requires_grad}\n",
    "    return norms\n",
    "\n",
    "print(\"Weight Norms with weight_decay:\", compute_weight_norms(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) 2. Implement L1 regularization on cat-dog classification neural network. Train the model on the dataset, and observe the impact of the regularization on the weight parameters. (Do not use data augmentation).\n",
    "a. L1 regularization using optimizer’s weight decay\n",
    "b. L1regularization using loop to find L1 norm of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer (without weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# L1 Regularization parameter\n",
    "lambda_l1 = 1e-4  # Adjust this value to control regularization strength\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute L1 norm manually\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += lambda_l1 * l1_norm  # Add L1 regularization term\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3)  Implement dropout regularization on cat-dog classification neural network. Train the model with and without dropout on a dataset, and compare the performance and overfitting tendencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, use_dropout=False):\n",
    "        super(CatDogCNN, self).__init__()\n",
    "        self.use_dropout = use_dropout\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.relu(self.fc1(x))\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)  # Apply dropout only if enabled\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) Implement your own version of the dropout layer by using Bernoulli distribution and compare\n",
    "the performance with the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(CustomDropout, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:  # No dropout in evaluation mode\n",
    "            return x\n",
    "        # Generate Bernoulli mask (same shape as x)\n",
    "        mask = torch.bernoulli(torch.full_like(x, self.p)) / self.p\n",
    "        return x * mask  # Apply dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5) Implement early stopping as a form of regularization. Train a neural network and monitor the validation loss. Stop training when the validation loss starts increasing, and compare the performance with a model trained without early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.5658, Val Loss: 0.6800\n",
      "Epoch 2: Train Loss: 0.6118, Val Loss: 0.6224\n",
      "Epoch 3: Train Loss: 0.5235, Val Loss: 0.6462\n",
      "Epoch 4: Train Loss: 0.4373, Val Loss: 0.6012\n",
      "Epoch 5: Train Loss: 0.3630, Val Loss: 0.6134\n",
      "Epoch 6: Train Loss: 0.2964, Val Loss: 0.6511\n",
      "Epoch 7: Train Loss: 0.2354, Val Loss: 0.6603\n",
      "Epoch 8: Train Loss: 0.1859, Val Loss: 0.7334\n",
      "Epoch 9: Train Loss: 0.1487, Val Loss: 0.7196\n",
      "Early stopping triggered after 9 epochs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations (NO data augmentation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder(root=\"/home/mustafa/dllab/w7prac/cats_and_dogs_filtered/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"/home/mustafa/dllab/w7prac/cats_and_dogs_filtered/validation\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 2)  # Assuming image size is 128x128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Stop if no improvement for 5 epochs\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "best_model_weights = None\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_weights = model.state_dict()  # Save best model weights\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "            model.load_state_dict(best_model_weights)  # Restore best model\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

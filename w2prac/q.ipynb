{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw Computation Graph and work out the gradient dz/da by following the path\n",
    "back from z to a and compare the result with the analytical gradient.\n",
    "x = 2*a + 3*b\n",
    "y = 5*a*a + 3*b*b*b\n",
    "z = 2*x + 3*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient dz/da: tensor(64.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True)  # Example value for a\n",
    "b = torch.tensor(3.0, requires_grad=False) # b does not require gradient\n",
    "x = 2 * a + 3 * b\n",
    "y = 5 * a**2 + 3 * b**3\n",
    "z = 2 * x + 3 * y\n",
    "\n",
    "z.backward()  # This computes all the gradients\n",
    "\n",
    "# Output the gradient dz/da\n",
    "print(\"Gradient dz/da:\", a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For the following Computation Graph, work out the gradient da/dw by following the\n",
    "path back from a to w and compare the result with the analytical gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient da/dw: tensor(2.)\n",
      "Gradient da/dw manually: tensor(2., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)  # Example value for w\n",
    "x = torch.tensor(2.0,requires_grad=True)  # Example value for x\n",
    "b = torch.tensor(1.0,requires_grad=True)  # Example value for b\n",
    "\n",
    "def manual(x1):\n",
    "    return max(x1,0)\n",
    "# Forward pass\n",
    "u = w * x\n",
    "v = u + b\n",
    "a = torch.relu(v)\n",
    "\n",
    "# Compute the gradient da/dw\n",
    "a.backward()\n",
    "\n",
    "# Output the gradient da/dw\n",
    "print(\"Gradient da/dw:\", w.grad)\n",
    "print(\"Gradient da/dw manually:\", manual(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Repeat the Problem 2 using Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "Gradient da/dw: tensor(0.0904)\n",
      "Gradient da/dw manually: tensor(0.0904)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# Define the variables\n",
    "w = torch.tensor(1.0, requires_grad=True)  # Example value for w\n",
    "x = torch.tensor(2.0)  # Example value for x\n",
    "print(x.shape)\n",
    "b = torch.tensor(1.0)  # Example value for b\n",
    "def manual(w,x,b):\n",
    "    return x*(1/(1+math.exp(-w*x-b)))*(1-(1/(1+math.exp(-w*x-b))))\n",
    "# Forward pass\n",
    "u = w * x\n",
    "v = u + b\n",
    "a = torch.sigmoid(v)  # Sigmoid activation function\n",
    "\n",
    "# Compute the gradient da/dw\n",
    "a.backward()\n",
    "\n",
    "# Output the gradient da/dw\n",
    "print(\"Gradient da/dw:\", w.grad)\n",
    "print(\"Gradient da/dw manually:\", manual(w,x,b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Verify that the gradients provided by PyTorch match with the analytical gradients of\n",
    "the function f= exp(-x2-2x-sin(x)) w.r.t x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch computed gradient: tensor(-0.0974)\n",
      "Analytical gradient: tensor(-0.0974, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)  # Example value for x\n",
    "\n",
    "f_x = torch.exp(-x**2 - 2*x - torch.sin(x))\n",
    "\n",
    "f_x.backward()\n",
    "\n",
    "print(\"PyTorch computed gradient:\", x.grad)\n",
    "\n",
    "# Analytical gradient: exp(-x^2 - 2x - sin(x)) * (-2x - 2 - cos(x))\n",
    "analytical_grad = torch.exp(-x**2 - 2*x - torch.sin(x)) * (-2*x - 2 - torch.cos(x))\n",
    "\n",
    "# Output the analytical gradient\n",
    "print(\"Analytical gradient:\", analytical_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compute gradient for the function y=8x4+ 3x3 +7x\n",
    "2+6x+3 and verify the gradients\n",
    "provided by PyTorch with the analytical gradients. A snapshot of the Python code is\n",
    "provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch computed gradient: tensor(326.)\n",
      "Analytical gradient: tensor(326., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)  # Example value for x\n",
    "\n",
    "# Define the function y = 8x^4 + 3x^3 + 7x^2 + 6x + 3\n",
    "y = 8*x**4 + 3*x**3 + 7*x**2 + 6*x + 3\n",
    "y.backward()\n",
    "\n",
    "print(\"PyTorch computed gradient:\", x.grad)\n",
    "\n",
    "# Analytical gradient: 32x^3 + 9x^2 + 14x + 6\n",
    "analytical_grad = 32*x**3 + 9*x**2 + 14*x + 6\n",
    "\n",
    "# Output the analytical gradient\n",
    "print(\"Analytical gradient:\", analytical_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

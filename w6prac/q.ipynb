{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perform classification on FashionMNIST, fashion apparels dataset, using a pretrained model which is trained on MNIST handwritten digit classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.2484, Accuracy: 91.99%\n",
      "Epoch 2/5, Loss: 0.1106, Accuracy: 96.62%\n",
      "Epoch 3/5, Loss: 0.0862, Accuracy: 97.30%\n",
      "Epoch 4/5, Loss: 0.0749, Accuracy: 97.72%\n",
      "Epoch 5/5, Loss: 0.0669, Accuracy: 97.96%\n",
      "Test Accuracy: 98.73%\n",
      "Optimized model saved to mmodel.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# Define a more complex CNN model with Batch Normalization and ReLU activation\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization after conv1\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization after conv2\n",
    "        # Convolutional layer 3 (additional layer for more complexity)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization after conv3\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout regularization to reduce overfitting\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "            # Conv1 -> BatchNorm -> ReLU -> MaxPool\n",
    "            x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "            # Conv2 -> BatchNorm -> ReLU -> MaxPool\n",
    "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "            # Conv3 -> BatchNorm -> ReLU -> MaxPool\n",
    "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "            # Flatten the tensor to feed into fully connected layers\n",
    "            x = x.view(-1, 128 * 3 * 3)  # This assumes the size is 128x7x7\n",
    "\n",
    "            # Fully connected layer 1 -> ReLU activation\n",
    "            x = torch.relu(self.fc1(x))\n",
    "\n",
    "            # Apply dropout after the fully connected layer to prevent overfitting\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            # Fully connected layer 2 (output layer)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "# Set up data loaders for FashionMNIST with data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # Random rotation between -10 and 10 degrees\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Use FashionMNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, optimizer, and learning rate scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier().to(device)  # Move the model to the GPU or CPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce the learning rate by a factor of 0.5 every 5 epochs\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 5  # Increased number of epochs for better training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward(\n",
    "\n",
    "\n",
    ")  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    scheduler.step()  # Update the learning rate\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"/home/mustafa/dllab/week6/model.pth\")\n",
    "print(\"Optimized model saved to mmodel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "/tmp/ipykernel_17859/1896746872.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  path = torch.load(\"/home/mustafa/dllab/week6/model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([32, 1, 3, 3])\n",
      "conv1.bias \t torch.Size([32])\n",
      "bn1.weight \t torch.Size([32])\n",
      "bn1.bias \t torch.Size([32])\n",
      "bn1.running_mean \t torch.Size([32])\n",
      "bn1.running_var \t torch.Size([32])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "conv2.weight \t torch.Size([64, 32, 3, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "bn2.weight \t torch.Size([64])\n",
      "bn2.bias \t torch.Size([64])\n",
      "bn2.running_mean \t torch.Size([64])\n",
      "bn2.running_var \t torch.Size([64])\n",
      "bn2.num_batches_tracked \t torch.Size([])\n",
      "conv3.weight \t torch.Size([128, 64, 3, 3])\n",
      "conv3.bias \t torch.Size([128])\n",
      "bn3.weight \t torch.Size([128])\n",
      "bn3.bias \t torch.Size([128])\n",
      "bn3.running_mean \t torch.Size([128])\n",
      "bn3.running_var \t torch.Size([128])\n",
      "bn3.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([512, 1152])\n",
      "fc1.bias \t torch.Size([512])\n",
      "fc2.weight \t torch.Size([10, 512])\n",
      "fc2.bias \t torch.Size([10])\n",
      "\n",
      "The overall accuracy is 4.75%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the CNN model (same as in MNIST)\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization after conv1\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization after conv2\n",
    "        # Convolutional layer 3 (additional layer for more complexity)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization after conv3\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout regularization to reduce overfitting\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "            # Conv1 -> BatchNorm -> ReLU -> MaxPool\n",
    "            x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "            # Conv2 -> BatchNorm -> ReLU -> MaxPool\n",
    "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "            # Conv3 -> BatchNorm -> ReLU -> MaxPool\n",
    "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "            # Flatten the tensor to feed into fully connected layers\n",
    "            x = x.view(-1, 128 * 3 * 3)  # This assumes the size is 128x3x3\n",
    "\n",
    "            # Fully connected layer 1 -> ReLU activation\n",
    "            x = torch.relu(self.fc1(x))\n",
    "\n",
    "            # Apply dropout after the fully connected layer to prevent overfitting\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            # Fully connected layer 2 (output layer)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "# Step 1: Load FashionMNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "mnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(mnist_testset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Step 2: Load the pre-trained model from disk\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = torch.load(\"/home/mustafa/dllab/week6/model.pth\")\n",
    "\n",
    "# Load the entire model (which includes both architecture and trained parameters)\n",
    "model=CNNClassifier()\n",
    "model.load_state_dict(path)\n",
    "model.to(device)  # Move the model to the correct device (GPU/CPU)\n",
    "\n",
    "# Step 3: Print model state_dict (inspect parameters)\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict().keys():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "print()\n",
    "\n",
    "# Step 4: Evaluate the model on the FashionMNIST test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Perform forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Get predicted class label (highest value in the output layer)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "    # Calculate total number of labels\n",
    "    total += labels.size(0)\n",
    "\n",
    "    # Calculate total correct predictions\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = 100.0 * correct / total\n",
    "print(\"The overall accuracy is {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Learn the AlexNet architecture and apply transfer learning to perform the classification task. Using the pre-trained AlexNet, classify images from the cats_and_dogs_filtered dataset downloaded from the below link. Finetune the classifier given in AlexNet as a two- class classifier. Perform pre-processing of images as per the requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory: cats_and_dogs_filtered/train\n",
      "Validation directory: cats_and_dogs_filtered/validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/mustafa/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1682, Accuracy: 93.05%\n",
      "Epoch [2/10], Loss: 0.0975, Accuracy: 96.40%\n",
      "Epoch [3/10], Loss: 0.0842, Accuracy: 96.90%\n",
      "Epoch [4/10], Loss: 0.0605, Accuracy: 97.75%\n",
      "Epoch [5/10], Loss: 0.0566, Accuracy: 97.70%\n",
      "Epoch [6/10], Loss: 0.0584, Accuracy: 97.75%\n",
      "Epoch [7/10], Loss: 0.0497, Accuracy: 98.30%\n",
      "Epoch [8/10], Loss: 0.0436, Accuracy: 98.30%\n",
      "Epoch [9/10], Loss: 0.0459, Accuracy: 98.35%\n",
      "Epoch [10/10], Loss: 0.0354, Accuracy: 99.00%\n",
      "Validation Accuracy: 96.30%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "base_dir = 'cats_and_dogs_filtered'  # Adjust based on your extracted folder structure\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "valid_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "print(\"Train directory:\", train_dir)\n",
    "print(\"Validation directory:\", valid_dir)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(227),  # Crop the image to 227x227\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Pre-trained AlexNet normalization\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(valid_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=2)  # Change to 2 output classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "epochs = 10  # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during validation\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement check points in PyTorch by saving model state_dict, optimizer state_dict, epochs and loss during training so that the training can be resumed at a later point. Also, illustrate the use of check point to save the best found parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1772\n",
      "Best model saved with loss 0.1772\n",
      "Epoch [2/10], Loss: 0.0499\n",
      "Checkpoint saved at epoch 2\n",
      "Best model saved with loss 0.0499\n",
      "Epoch [3/10], Loss: 0.0364\n",
      "Best model saved with loss 0.0364\n",
      "Epoch [4/10], Loss: 0.0310\n",
      "Checkpoint saved at epoch 4\n",
      "Best model saved with loss 0.0310\n",
      "Epoch [5/10], Loss: 0.0248\n",
      "Best model saved with loss 0.0248\n",
      "Epoch [6/10], Loss: 0.0195\n",
      "Checkpoint saved at epoch 6\n",
      "Best model saved with loss 0.0195\n",
      "Epoch [7/10], Loss: 0.0190\n",
      "Best model saved with loss 0.0190\n",
      "Epoch [8/10], Loss: 0.0156\n",
      "Checkpoint saved at epoch 8\n",
      "Best model saved with loss 0.0156\n",
      "Epoch [9/10], Loss: 0.0144\n",
      "Best model saved with loss 0.0144\n",
      "Epoch [10/10], Loss: 0.0123\n",
      "Checkpoint saved at epoch 10\n",
      "Best model saved with loss 0.0123\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # This will be updated dynamically\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions and pooling\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # Dynamically calculate the size for flattening\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "checkpoint_path = './checkpoints/checkpoint.pt'\n",
    "start_epoch = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    start_epoch = checkpoint['last_epoch']\n",
    "    best_loss = checkpoint['last_loss']\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint = {\n",
    "            \"last_loss\": avg_loss,\n",
    "            \"last_epoch\": epoch + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_path = './checkpoints/best_model.pt'\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved with loss {best_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17859/2826842733.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 10...\n",
      "Epoch [11/13], Loss: 0.0112\n",
      "Best model saved with loss 0.0112\n",
      "Epoch [12/13], Loss: 0.0112\n",
      "Checkpoint saved at epoch 12\n",
      "Best model saved with loss 0.0112\n",
      "Epoch [13/13], Loss: 0.0104\n",
      "Best model saved with loss 0.0104\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the CNN model (same as above)\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 3 * 3)  # Flattening the output of the conv layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Checkpoint directory\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Check for a pre-existing checkpoint\n",
    "checkpoint_path = './checkpoints/checkpoint.pt'\n",
    "start_epoch = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    start_epoch = checkpoint['last_epoch']\n",
    "    best_loss = checkpoint['last_loss']\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "# Training loop (same as saving the checkpoint)\n",
    "num_epochs = 13  # Total number of epochs for training\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint = {\n",
    "            \"last_loss\": avg_loss,\n",
    "            \"last_epoch\": epoch + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_path = './checkpoints/best_model.pt'\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved with loss {best_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
